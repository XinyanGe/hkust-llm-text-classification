# LLM-based Text Classification with Soft-Prompt and Determined Fine-Tuning  
**Research Assistant Project @ HKUST (with Prof. Miaozhe Han)**  

This project explores soft-prompt tuning and determined fine-tuning methods for large language model (LLM) based text classification.  
I contributed to building a large-scale dataset pipeline (230k+ samples), implementing fine-tuning frameworks, and evaluating multiple models including LLaMA and GPT-3.5-turbo.

## Overview  
The research aims to improve classification performance and efficiency in LLMs through lightweight tuning methods.  
My role involved data preprocessing, prompt-based model adaptation, and result analysis under supervised training frameworks.

## Key Components  
- Data pipeline construction and cleaning (230k+ samples)  
- Implementation of soft-prompt tuning and determined fine-tuning  
- Cross-model evaluation (LLaMA, GPT-3.5-turbo)  
- Comparative analysis of model performance and efficiency  

## Technologies  
- Python, PyTorch  
- Hugging Face Transformers  
- Pandas, NumPy  
- Determined AI (DFT framework)

## ðŸ“˜ Included Tools
### `get_abstract_2_multupdate3.py`  
A **high-performance Hugging Face Spaces data collector** developed during my RA work at HKUST.  
It supports **multi-threaded crawling**, **commit-level history tracking**, and **metadata extraction**,  
enabling large-scale analysis of **model repository evolution** and **reproducibility**.

**Usage:**
```bash
export HF_TOKEN="your_hf_token"
python get_abstract_2_multupdate3.py

## Note  
Due to research confidentiality, only non-sensitive parts of the project are shared.  
For more details, please contact me at [xg2280@nyu.edu](mailto:xg2280@nyu.edu).
