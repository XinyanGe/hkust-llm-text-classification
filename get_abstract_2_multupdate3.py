# -*- coding: utf-8 -*-
import os
import json
import time
import traceback
import threading
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import wraps

import pandas as pd
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from huggingface_hub import (
    HfApi,
    list_repo_tree,
    get_paths_info,
    repo_info,
    hf_hub_download,
    login,
)

# =======================
# é€šç”¨å·¥å…·
# =======================
class ThreadSafeProgress:
    """ä¼˜åŒ–çš„çº¿ç¨‹å®‰å…¨è¿›åº¦è·Ÿè¸ªå™¨"""
    def __init__(self, total: int):
        self.total = total
        self.completed = 0
        self.successful = 0
        self.failed = 0
        self.lock = threading.Lock()
        self.start_time = time.time()
        self.last_print = 0
    
    def update(self, success: bool = True):
        with self.lock:
            self.completed += 1
            if success:
                self.successful += 1
            else:
                self.failed += 1
            
            # é™åˆ¶æ‰“å°é¢‘ç‡ï¼Œé¿å… I/O æŠ–åŠ¨
            now = time.time()
            if now - self.last_print >= 3 or self.completed % 50 == 0 or self.completed == self.total:
                elapsed = now - self.start_time
                rate = self.completed / elapsed if elapsed > 0 else 0
                eta = (self.total - self.completed) / rate if rate > 0 else 0
                
                print(
                    f"ğŸ“Š è¿›åº¦: {self.completed}/{self.total} "
                    f"({self.completed/self.total*100:.1f}%) | "
                    f"âœ…{self.successful} âŒ{self.failed} | "
                    f"ğŸš€{rate:.1f}/s | ETA: {eta/60:.1f}min"
                )
                self.last_print = now


class SmartRateLimiter:
    """æ™ºèƒ½ä»¤ç‰Œæ¡¶é€Ÿç‡é™åˆ¶å™¨"""
    def __init__(self, calls_per_second: float, burst_size: int = 10):
        self.rate = calls_per_second
        self.burst_size = burst_size
        self.tokens = burst_size
        self.last_refill = time.time()
        self.lock = threading.Lock()
    
    def acquire(self):
        with self.lock:
            now = time.time()
            # è¡¥å……ä»¤ç‰Œ
            elapsed = now - self.last_refill
            self.tokens = min(self.burst_size, self.tokens + elapsed * self.rate)
            self.last_refill = now
            
            if self.tokens >= 1:
                self.tokens -= 1
                return True
            else:
                # éœ€è¦ç­‰å¾…çš„æ—¶é—´
                wait_time = (1 - self.tokens) / self.rate
                return wait_time


def smart_rate_limit(calls_per_second: float, burst_size: int = 10):
    """æ™ºèƒ½é€Ÿç‡é™åˆ¶è£…é¥°å™¨"""
    limiter = SmartRateLimiter(calls_per_second, burst_size)
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            result = limiter.acquire()
            if result is not True:  # éœ€è¦ç­‰å¾…
                time.sleep(result)
            return func(*args, **kwargs)
        return wrapper
    return decorator


# =======================
# ä¸»æ”¶é›†å™¨
# =======================
class HighPerformanceSpaceCollector:
    """é«˜æ€§èƒ½ HuggingFace Spaces æ•°æ®æ”¶é›†å™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, hf_token: str, output_dir: str = "./spaces_data",
                 max_workers: int = 20, file_workers: int = 8):
        self.hf_token = hf_token
        self.output_dir = output_dir
        self.max_workers = max_workers
        self.file_workers = file_workers  # å•ä¸ª space å†…æ–‡ä»¶å¤„ç†å¹¶å‘æ•°

        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # æ—¥å¿—ï¼ˆæ–‡ä»¶é‡Œå°½é‡å®‰é™ï¼‰
        logging.basicConfig(
            level=logging.WARNING,
            format="%(asctime)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler(Path(output_dir) / "collection.log", encoding="utf-8")],
        )
        self.logger = logging.getLogger(__name__)

        # çº¿ç¨‹æœ¬åœ°å­˜å‚¨
        self.local_data = threading.local()

        # ç™»å½• Hugging Face
        try:
            if self.hf_token and self.hf_token.strip():
                login(token=self.hf_token)
            print("âœ… HuggingFace ç™»å½•æˆåŠŸ")
        except Exception as e:
            print(f"âŒ HuggingFace ç™»å½•å¤±è´¥: {e}")
            raise

    # ---------- å†…éƒ¨å·¥å…· ----------
    def _hf_api(self) -> HfApi:
        if not hasattr(self.local_data, "hf_api"):
            self.local_data.hf_api = HfApi()
        return self.local_data.hf_api

    def _get_optimized_session(self):
        """è‹¥åç»­éœ€è¦ HTTP è°ƒç”¨ï¼Œå¯ä½¿ç”¨è¿™ä¸ªå¸¦è¿æ¥æ± å’Œé‡è¯•çš„ sessionï¼ˆå½“å‰å…ƒæ•°æ®å·²æ”¹ç”¨ repo_infoï¼Œä¸å¼ºä¾èµ–ï¼‰"""
        if not hasattr(self.local_data, "session"):
            session = requests.Session()
            retry_strategy = Retry(
                total=3, backoff_factor=0.3, status_forcelist=[429, 500, 502, 503, 504]
            )
            adapter = HTTPAdapter(
                max_retries=retry_strategy, pool_connections=20, pool_maxsize=50, pool_block=False
            )
            session.mount("http://", adapter)
            session.mount("https://", adapter)
            session.headers.update({
                "Authorization": f"Bearer {self.hf_token}",
                "User-Agent": "HighPerformanceSpaceCollector/2.0",
                "Connection": "keep-alive",
                "Accept-Encoding": "gzip, deflate",
            })
            self.local_data.session = session
        return self.local_data.session

    # ---------- æ•°æ®åŠ è½½ ----------
    def load_spaces_from_csv(self, csv_path: str) -> List[str]:
        """ä¼˜åŒ–çš„ CSV åŠ è½½ï¼šè‡ªåŠ¨è¯†åˆ«åˆ—åï¼Œå¦åˆ™å–ç¬¬ä¸€åˆ—"""
        try:
            df = pd.read_csv(csv_path, dtype=str, na_filter=False)
            possible_columns = ["space_name", "name", "spaces", "id", "space"]
            spaces = []
            for col in possible_columns:
                if col in df.columns:
                    spaces = df[col].tolist()
                    print(f"ğŸ“Š ä»åˆ— '{col}' è¯»å–åˆ° {len(spaces)} ä¸ª spaces")
                    break
            if not spaces and len(df.columns) > 0:
                spaces = df.iloc[:, 0].tolist()
                print(f"ğŸ“Š ä½¿ç”¨ç¬¬ä¸€åˆ—è¯»å–åˆ° {len(spaces)} ä¸ª spaces")
            spaces = [s.strip() for s in spaces if s and str(s).strip()]
            return spaces
        except Exception as e:
            self.logger.error(f"âŒ åŠ è½½ CSV æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def create_space_folder(self, space_name: str) -> str:
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        safe = "".join(c if c.isalnum() or c in "-_." else "_" for c in space_name)
        base = Path(self.output_dir) / safe
        for sub in ["app_files", "readme_files", "metadata", "other_files"]:
            (base / sub).mkdir(parents=True, exist_ok=True)
        return str(base)

    # ---------- è‡ªåŠ¨å‘ç°ç›®æ ‡æ–‡ä»¶ ----------
    @smart_rate_limit(2.0, burst_size=8)
    def discover_target_files(self, space_name: str) -> List[str]:
        """
        æšä¸¾ä»“åº“æ–‡ä»¶ï¼ŒæŒ‘é€‰å¸¸è§å…¥å£/å…³æ³¨æ–‡ä»¶ï¼š
        - README.md / README.MD
        - app.py / app/app.py / src/app.py
        - main.py / src/main.py
        æ‰¾ä¸åˆ°æ—¶å›é€€åˆ° ["app.py", "README.md"]
        """
        try:
            tree = list_repo_tree(
                repo_id=space_name,
                repo_type="space",
                recursive=True,
                expand=False
            )
            candidates = set()
            for item in tree:
                p = getattr(item, "path", "") or ""
                low = p.lower()
                # å…³å¿ƒçš„æ–‡ä»¶
                if low.endswith("readme.md"):
                    candidates.add(p)
                elif low.endswith("/app.py") or low == "app.py":
                    candidates.add(p)
                elif low.endswith("/main.py") or low == "main.py":
                    candidates.add(p)
                # è‹¥è¿˜æœ‰å…¶ä»–ä½ æƒ³è·Ÿè¸ªçš„æ–‡ä»¶ï¼Œå¯æŒ‰éœ€è¡¥å……
            # æå‡å¯ç”¨æ€§ï¼šä¼˜å…ˆ READMEï¼Œç„¶å app/main
            if not candidates:
                return ["app.py", "README.md"]
            # æ’åºï¼šå›ºå®šè¾“å‡ºé¡ºåºä¾¿äºç¨³å®š
            ordered = []
            for name in ["README.md", "Readme.md", "readme.md",
                         "app.py", "app/app.py", "src/app.py",
                         "main.py", "src/main.py"]:
                if name in candidates:
                    ordered.append(name)
            # æŠŠéæ ‡å‡†ä½†åŒ¹é…çš„ä¹Ÿé™„ä¸Š
            others = sorted([c for c in candidates if c not in ordered])
            return ordered + others
        except Exception:
            return ["app.py", "README.md"]

    # ---------- å†å²ä¸å…ƒæ•°æ® ----------
    @smart_rate_limit(1.5, burst_size=6)
    def _list_repo_commits(self, space_name: str):
        """åˆ—å‡ºç©ºé—´ä»“åº“ commit å†å²ï¼ˆå€’åºï¼šæ–°->æ—§ï¼‰"""
        api = self._hf_api()
        return api.list_repo_commits(
            repo_id=space_name,
            repo_type="space",
            revision=None,      # é»˜è®¤ main
            formatted=False
        )

    @smart_rate_limit(3.0, burst_size=10)
    def _file_changed_in_commit(self, space_name: str, filename: str, commit_id: str,
                                last_blob_id: Optional[str]) -> Tuple[bool, Optional[str]]:
        """
        æ£€æŸ¥æŸ commit ä¸‹æŒ‡å®šæ–‡ä»¶çš„ blob_id æ˜¯å¦å˜åŒ–ï¼š
        - æ–‡ä»¶å­˜åœ¨ï¼Œä¸” blob_id ä¸ last_blob_id ä¸åŒ => å˜åŒ–
        - æ–‡ä»¶ä¸å­˜åœ¨æˆ–ç›¸åŒ => ä¸å˜åŒ–
        """
        try:
            infos = get_paths_info(
                repo_id=space_name,
                paths=[filename],
                revision=commit_id,
                repo_type="space",
                expand=False
            )
            for info in infos:
                if getattr(info, "path", None) == filename:
                    blob = getattr(info, "blob_id", None)
                    if blob:
                        if last_blob_id is None or blob != last_blob_id:
                            return True, blob
                        return False, blob
            # æœªæ‰¾åˆ°è¯¥æ–‡ä»¶ï¼ˆæ­¤ç‰ˆæœ¬ä¸å­˜åœ¨ï¼‰
            return False, last_blob_id
        except Exception:
            # 404 / æƒé™ç­‰å¼‚å¸¸ï¼šå½“ä½œæ— å˜åŒ–ï¼Œæ²¿ç”¨ last_blob_id
            return False, last_blob_id

    @smart_rate_limit(2.0, burst_size=8)
    def get_file_commits(self, space_name: str, filename: str) -> List[Tuple[str, datetime]]:
        """
        æ­£ç¡®çš„â€œæ–‡ä»¶çº§å†å²â€å®ç°ï¼š
        1) å–ä»“åº“ commit åˆ—è¡¨ï¼ˆæ–°->æ—§ï¼‰
        2) å€’åºéå†ï¼ˆæ—§->æ–°ï¼‰æ¯”è¾ƒ blob_idï¼Œæ•æ‰å‘ç”Ÿå˜åŒ–çš„æäº¤
        è¿”å› [(commit_id, created_at), ...]ï¼ˆæ—§->æ–°ï¼‰
        """
        try:
            commits = self._list_repo_commits(space_name)
        except Exception:
            return []

        # å€’åºï¼ˆæ—§ -> æ–°ï¼‰æ¯”è¾ƒ
        commits_sorted = list(reversed(commits))
        results: List[Tuple[str, datetime]] = []
        last_blob_id: Optional[str] = None

        for c in commits_sorted:
            commit_id = getattr(c, "commit_id", None) or getattr(c, "oid", None)
            created_at = getattr(c, "created_at", None)
            # created_at å¯èƒ½æ˜¯ strï¼Œä¹Ÿå¯èƒ½æ˜¯ datetimeï¼›ç»Ÿä¸€æˆ datetime
            if isinstance(created_at, str):
                try:
                    created_at = datetime.fromisoformat(created_at.replace("Z", "+00:00"))
                except Exception:
                    created_at = datetime.now(timezone.utc)

            if not commit_id or not created_at:
                continue

            changed, last_blob_id = self._file_changed_in_commit(
                space_name, filename, commit_id, last_blob_id
            )
            if changed:
                results.append((commit_id, created_at))

        return results  # å·²æŒ‰æ—§->æ–°

    def get_monthly_commits(self, commits: List[Tuple[str, datetime]]) -> List[Tuple[str, datetime]]:
        """æŒ‰æœˆæŠ½å–æœ€è¿‘ä¸€æ¬¡å˜æ›´"""
        if not commits:
            return []
        monthly: Dict[Tuple[int, int], Tuple[str, datetime]] = {}
        for commit_id, cdate in commits:
            key = (cdate.year, cdate.month)
            hit = monthly.get(key)
            if (hit is None) or (cdate > hit[1]):
                monthly[key] = (commit_id, cdate)
        return sorted(monthly.values(), key=lambda x: x[1])

    @smart_rate_limit(2.0, burst_size=8)
    def get_space_metadata(self, space_name: str, commit_id: str) -> Optional[Dict]:
        """ç”¨æ–‡æ¡£åŒ–çš„ repo_info è·å–æŒ‡å®š revision çš„ç©ºé—´ä¿¡æ¯"""
        try:
            info = repo_info(
                repo_id=space_name,
                repo_type="space",
                revision=commit_id
            )
            return {
                "space_name": space_name,
                "commit_id": commit_id,
                "sdk": getattr(info, "sdk", None),
                "likes": getattr(info, "likes", 0),
                "created_at": getattr(info, "created_at", None),
                "updated_at": getattr(info, "last_modified", None),
                "tags": getattr(info, "tags", []) or [],
                "models": getattr(info, "models", []) or [],
                "datasets": getattr(info, "datasets", []) or [],
                "card_data": getattr(info, "card_data", {}) or {},
                "collection_time": datetime.now(timezone.utc).isoformat(),
            }
        except Exception:
            return None

    # ---------- ä¸‹è½½ ----------
    @smart_rate_limit(3.0, burst_size=10)
    def download_file_version(self, space_name: str, filename: str, commit_id: str,
                              commit_date: datetime, space_folder: str) -> Optional[Dict]:
        """ä¸‹è½½ç‰¹å®š commit çš„æ–‡ä»¶ç‰ˆæœ¬"""
        try:
            subfolder_map = {"app.py": "app_files", "README.md": "readme_files"}
            # æ ¹æ®çœŸå®è·¯å¾„å½’ç±»ï¼ˆè‹¥ path ä¸­åŒ…å« app.py æˆ– main.pyï¼Œä¹Ÿå½’åˆ° app_filesï¼‰
            low = filename.lower()
            if low.endswith("/app.py") or low.endswith("app.py") or low.endswith("main.py"):
                subfolder = "app_files"
            elif low.endswith("readme.md"):
                subfolder = "readme_files"
            else:
                subfolder = subfolder_map.get(filename, "other_files")

            local_dir = Path(space_folder) / subfolder
            local_dir.mkdir(parents=True, exist_ok=True)

            downloaded_path = hf_hub_download(
                repo_id=space_name,
                repo_type="space",
                revision=commit_id,
                filename=filename,
                local_dir=str(local_dir),
                local_dir_use_symlinks=False
            )

            # å‘½åï¼šç”¨åŸå§‹è·¯å¾„æ›¿æ¢æ–œæ ï¼Œé¿å…å†²çª
            file_base = filename.replace("/", "__").replace(".", "_")
            date_str = commit_date.strftime("%Y%m%d")
            file_ext = Path(filename).suffix
            new_filename = f"{file_base}_{commit_id[:7]}_{date_str}{file_ext or ''}"
            new_path = local_dir / new_filename

            if Path(downloaded_path).exists():
                # è‹¥ä¸‹è½½çš„æ–‡ä»¶åä¸ new_path ä¸åŒåˆ™é‡å‘½å
                if str(downloaded_path) != str(new_path):
                    Path(downloaded_path).rename(new_path)
                return {
                    "file_path": str(new_path),
                    "commit_id": commit_id,
                    "commit_date": commit_date.isoformat(),
                    "file_size": new_path.stat().st_size,
                    "source_file": filename
                }
        except Exception:
            pass
        return None

    # ---------- å•ç©ºé—´å¤„ç† ----------
    def process_file_parallel(self, space_name: str, filename: str,
                              monthly_commits: List[Tuple[str, datetime]],
                              space_folder: str) -> Tuple[List[Dict], List[Dict]]:
        """å¹¶è¡Œå¤„ç†æŸä¸ªæ–‡ä»¶çš„æ‰€æœ‰æœˆåº¦ç‰ˆæœ¬ï¼šä¸‹è½½ + å…ƒæ•°æ®"""
        downloaded_files: List[Dict] = []
        metadata_list: List[Dict] = []

        max_concurrent = min(self.file_workers, len(monthly_commits))
        if max_concurrent <= 0:
            return downloaded_files, metadata_list

        with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
            # ä¸‹è½½ä»»åŠ¡
            download_futs = {
                executor.submit(
                    self.download_file_version, space_name, filename, cid, cdate, space_folder
                ): (cid, cdate)
                for cid, cdate in monthly_commits
            }
            # å…ƒæ•°æ®ä»»åŠ¡
            meta_futs = {
                executor.submit(self.get_space_metadata, space_name, cid): (cid, cdate)
                for cid, cdate in monthly_commits
            }

            # æ”¶é›†ä¸‹è½½
            for fut in as_completed(download_futs):
                try:
                    res = fut.result()
                    if res:
                        downloaded_files.append(res)
                except Exception:
                    pass

            # æ”¶é›†å…ƒæ•°æ®
            for fut in as_completed(meta_futs):
                cid, cdate = meta_futs[fut]
                try:
                    meta = fut.result()
                    if meta:
                        meta["source_file"] = filename
                        meta["file_commit_date"] = cdate.isoformat()
                        metadata_list.append(meta)
                except Exception:
                    pass

        return downloaded_files, metadata_list

    def process_single_space(self, space_name: str) -> Dict:
        """å¤„ç†å•ä¸ª spaceï¼šå‘ç°æ–‡ä»¶ -> å–æ–‡ä»¶å†å² -> æœˆåº¦æŠ½æ · -> ä¸‹è½½ä¸å…ƒæ•°æ®"""
        try:
            space_folder = self.create_space_folder(space_name)
            space_data: Dict = {
                "space_name": space_name,
                "space_folder": space_folder,
                "processing_time": datetime.now(timezone.utc).isoformat(),
                "files_data": {},
                "metadata_history": [],
                "status": "processing",
            }

            # å‘ç°å€™é€‰æ–‡ä»¶
            target_files = self.discover_target_files(space_name)
            if not target_files:
                target_files = ["app.py", "README.md"]

            for filename in target_files:
                commits = self.get_file_commits(space_name, filename)  # æ–‡ä»¶çº§å˜æ›´å†å²ï¼ˆæ—§->æ–°ï¼‰
                if not commits:
                    # è¯¥æ–‡ä»¶åœ¨å†å²ä¸Šä»æœªå­˜åœ¨æˆ–æœªå˜æ›´
                    continue

                # monthly_commits = self.get_monthly_commits(commits)
                monthly_commits = commits  # å…³é—­æœˆé‡‡æ ·ï¼šä½¿ç”¨å®Œæ•´æäº¤å†å²
                if not monthly_commits:
                    continue

                downloaded_files, metadata_list = self.process_file_parallel(
                    space_name, filename, monthly_commits, space_folder
                )

                space_data["files_data"][filename] = {
                    "total_change_commits": len(commits),
                    "monthly_commits_count": len(monthly_commits),
                    "downloaded_files": downloaded_files,
                    "commit_timeline": [(cid, cdate.isoformat()) for cid, cdate in monthly_commits],
                }

                space_data["metadata_history"].extend(metadata_list)

            space_data["status"] = "completed"
            self._save_space_data(space_folder, space_data)
            return space_data

        except Exception as e:
            return {
                "space_name": space_name,
                "status": "failed",
                "error": str(e),
                "processing_time": datetime.now(timezone.utc).isoformat(),
            }

    # ---------- ä¿å­˜ä¸æŠ¥å‘Š ----------
    def _save_space_data(self, space_folder: str, space_data: Dict):
        try:
            with open(Path(space_folder) / "space_data.json", "w", encoding="utf-8") as f:
                json.dump(space_data, f, indent=2, ensure_ascii=False, default=str)

            summary = {
                "space_name": space_data["space_name"],
                "total_files": len(space_data["files_data"]),
                "total_versions": sum(len(f["downloaded_files"]) for f in space_data["files_data"].values()),
                "metadata_count": len(space_data["metadata_history"]),
                "status": space_data["status"],
                "processing_time": space_data["processing_time"],
            }
            with open(Path(space_folder) / "summary.json", "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")

    def _generate_report(self, results: Dict):
        report_file = Path(self.output_dir) / "collection_report.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write("# ğŸš€ é«˜æ€§èƒ½ HuggingFace Spaces æ”¶é›†æŠ¥å‘Š\n\n")
            f.write(f"**å¼€å§‹æ—¶é—´**: {results['start_time']}\n")
            f.write(f"**ç»“æŸæ—¶é—´**: {results['end_time']}\n")
            f.write(f"**æ€»å¤„ç†æ—¶é—´**: {results['total_duration']:.1f} ç§’\n\n")
            f.write("## ğŸ“Š æ€§èƒ½ç»Ÿè®¡\n\n")
            f.write(f"- **æ€» spaces æ•°**: {results['total_spaces']}\n")
            f.write(f"- **æˆåŠŸæ”¶é›†**: {results['successful_count']}\n")
            f.write(f"- **æ”¶é›†å¤±è´¥**: {results['failed_count']}\n")
            f.write(f"- **æˆåŠŸç‡**: {results['success_rate']:.1f}%\n")
            f.write(f"- **å¹¶å‘çº¿ç¨‹æ•°**: {results['max_workers']}\n")
            if results["total_duration"] > 0:
                rate = results["successful_count"] / results["total_duration"]
                f.write(f"- **å¤„ç†é€Ÿç‡**: {rate:.2f} spaces/ç§’\n")

    # ---------- æ€»æ§ ----------
    def collect_all_spaces(self, csv_path: str) -> Dict:
        print("ğŸš€ å¯åŠ¨é«˜æ€§èƒ½æ•°æ®æ”¶é›†å™¨")
        print(f"ğŸ“Š CSV æ–‡ä»¶: {csv_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ§µ ä¸»çº¿ç¨‹æ•°: {self.max_workers}")
        print(f"ğŸ“„ æ–‡ä»¶å¤„ç†å¹¶å‘: {self.file_workers}")

        spaces_list = self.load_spaces_from_csv(csv_path)
        if not spaces_list:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ spaces åˆ—è¡¨")
            return {}

        print(f"ğŸ“‹ å¾…å¤„ç† spaces æ•°é‡: {len(spaces_list)}")

        progress = ThreadSafeProgress(len(spaces_list))
        results: Dict = {
            "start_time": datetime.now(timezone.utc).isoformat(),
            "total_spaces": len(spaces_list),
            "max_workers": self.max_workers,
            "successful_spaces": [],
            "failed_spaces": [],
            "spaces_data": {},
        }

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            fut_map = {executor.submit(self.process_single_space, s): s for s in spaces_list}
            for fut in as_completed(fut_map):
                space_name = fut_map[fut]
                try:
                    res = fut.result()
                    results["spaces_data"][space_name] = res
                    if res.get("status") == "completed":
                        results["successful_spaces"].append(space_name)
                        progress.update(success=True)
                    else:
                        results["failed_spaces"].append(space_name)
                        progress.update(success=False)
                except Exception:
                    results["failed_spaces"].append(space_name)
                    progress.update(success=False)

        results.update({
            "end_time": datetime.now(timezone.utc).isoformat(),
            "successful_count": len(results["successful_spaces"]),
            "failed_count": len(results["failed_spaces"]),
            "success_rate": len(results["successful_spaces"]) / len(spaces_list) * 100,
            "total_duration": time.time() - progress.start_time,
        })

        with open(Path(self.output_dir) / "final_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        self._generate_report(results)
        return results


# =======================
# å…¥å£
# =======================
def main():
    """ä¸»å‡½æ•°ï¼šé…ç½®å¯æŒ‰éœ€ä¿®æ”¹"""
    CONFIG = {
        # æ¨èä½¿ç”¨ç¯å¢ƒå˜é‡ï¼š export HF_TOKEN=xxx
        "HF_TOKEN": os.environ.get("HF_TOKEN", "YOUR_HF_TOKEN"),
        "CSV_PATH": "spaces.csv",
        "OUTPUT_DIR": "spaces_data",
        "MAX_WORKERS": 14,    # ä¸»å¹¶å‘ï¼ˆæ³¨æ„ API é™æµï¼‰
        "FILE_WORKERS": 12,    # å• space å†…æ–‡ä»¶å¹¶å‘
    }

    print("âš¡ å¯åŠ¨é«˜æ€§èƒ½ HuggingFace Spaces æ”¶é›†å™¨")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”¥ å¹¶å‘é…ç½®: ä¸» {CONFIG['MAX_WORKERS']} / æ–‡ä»¶ {CONFIG['FILE_WORKERS']}")

    try:
        collector = HighPerformanceSpaceCollector(
            hf_token=CONFIG["HF_TOKEN"],
            output_dir=CONFIG["OUTPUT_DIR"],
            max_workers=CONFIG["MAX_WORKERS"],
            file_workers=CONFIG["FILE_WORKERS"],
        )
        t0 = time.time()
        results = collector.collect_all_spaces(CONFIG["CSV_PATH"])
        t1 = time.time()

        print("\n" + "=" * 60)
        print("ğŸ‰ æ”¶é›†å®Œæˆ")
        print("=" * 60)
        print(f"ğŸ“Š æ€» spaces: {results.get('total_spaces', 0)}")
        print(f"âœ… æˆåŠŸ: {results.get('successful_count', 0)}")
        print(f"âŒ å¤±è´¥: {results.get('failed_count', 0)}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {results.get('success_rate', 0):.1f}%")
        print(f"â±ï¸  æ€»è€—æ—¶: {results.get('total_duration', t1 - t0):.1f} ç§’")
        if results.get("successful_count", 0) > 0:
            rate = results.get("successful_count", 0) / max(results.get("total_duration", t1 - t0), 1e-6)
            print(f"ğŸš€ å¤„ç†é€Ÿç‡: {rate:.2f} spaces/ç§’")
        print(f"ğŸ“‚ æ•°æ®ä½ç½®: {CONFIG['OUTPUT_DIR']}")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå¤±è´¥: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    main()